{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Read multiline json file 1\ndata_df = spark.read.option(\"multiline\",\"false\") \\\n      .json(\"/FileStore/tables/JSON/examples.json\")\ndisplay(data_df) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a7ff580-45a3-413e-933f-eb3ef85b8081"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["5ee69d943260aab97ea0d58d"],null,null,null,null,null,null,null,null,null,null,null,null,"Pizza",null],[["5ee69e393260aab97ea0d58e"],null,null,null,null,null,null,null,null,null,null,null,null,"Delete me",null],[["5e5e9c470d33e9e8e3891b35"],210,20,"Classic Mexican tacos",["Brown beef","Add taco seasoning and water, mix","Bring to boil","Lower heat to simmer 5-10 minutes until desired consistency","Put meat in tacos"],[["ground beef (lean)",[1,"lbs"]],["taco seasoning",[2,"oz"]],["corn hard tacos",[12,"oz"]]],[1,415],2,10,[4,4,3,4,2,5,2,2,4,5],3.5,4,["mexican","quick","easy","ground beef"],"Tacos","Dinner"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_id","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"$oid\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"calories_per_serving","type":"\"long\"","metadata":"{}"},{"name":"cook_time","type":"\"long\"","metadata":"{}"},{"name":"desc","type":"\"string\"","metadata":"{}"},{"name":"directions","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"ingredients","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"quantity\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"amount\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"unit\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"},{"name":"likes","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"},{"name":"likes_count","type":"\"long\"","metadata":"{}"},{"name":"prep_time","type":"\"long\"","metadata":"{}"},{"name":"rating","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"},{"name":"rating_avg","type":"\"double\"","metadata":"{}"},{"name":"servings","type":"\"long\"","metadata":"{}"},{"name":"tags","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"title","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_id</th><th>calories_per_serving</th><th>cook_time</th><th>desc</th><th>directions</th><th>ingredients</th><th>likes</th><th>likes_count</th><th>prep_time</th><th>rating</th><th>rating_avg</th><th>servings</th><th>tags</th><th>title</th><th>type</th></tr></thead><tbody><tr><td>List(5ee69d943260aab97ea0d58d)</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Pizza</td><td>null</td></tr><tr><td>List(5ee69e393260aab97ea0d58e)</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Delete me</td><td>null</td></tr><tr><td>List(5e5e9c470d33e9e8e3891b35)</td><td>210</td><td>20</td><td>Classic Mexican tacos</td><td>List(Brown beef, Add taco seasoning and water, mix, Bring to boil, Lower heat to simmer 5-10 minutes until desired consistency, Put meat in tacos)</td><td>List(List(ground beef (lean), List(1, lbs)), List(taco seasoning, List(2, oz)), List(corn hard tacos, List(12, oz)))</td><td>List(1, 415)</td><td>2</td><td>10</td><td>List(4, 4, 3, 4, 2, 5, 2, 2, 4, 5)</td><td>3.5</td><td>4</td><td>List(mexican, quick, easy, ground beef)</td><td>Tacos</td><td>Dinner</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# directions columns contains Array lets try to flat it out...\n\n#Using SQL col() function\nfrom pyspark.sql.functions import col\ndata_df2= data_df[\"title\",\"directions\"].filter(col(\"title\")==\"Tacos\")\n\ndisplay(data_df2)\n\nfrom pyspark.sql.functions import explode\ndisplay(  data_df2.select(data_df2.title,explode(data_df2.directions))  ) # explode function used for flat it out...\n\ndata_df3=data_df2.select(data_df2.title,explode(data_df2.directions))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b5f941-5d13-4adf-a466-4b279ff20014"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Tacos",["Brown beef","Add taco seasoning and water, mix","Bring to boil","Lower heat to simmer 5-10 minutes until desired consistency","Put meat in tacos"]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"title","type":"\"string\"","metadata":"{}"},{"name":"directions","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>directions</th></tr></thead><tbody><tr><td>Tacos</td><td>List(Brown beef, Add taco seasoning and water, mix, Bring to boil, Lower heat to simmer 5-10 minutes until desired consistency, Put meat in tacos)</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Tacos","Brown beef"],["Tacos","Add taco seasoning and water, mix"],["Tacos","Bring to boil"],["Tacos","Lower heat to simmer 5-10 minutes until desired consistency"],["Tacos","Put meat in tacos"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"title","type":"\"string\"","metadata":"{}"},{"name":"col","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>col</th></tr></thead><tbody><tr><td>Tacos</td><td>Brown beef</td></tr><tr><td>Tacos</td><td>Add taco seasoning and water, mix</td></tr><tr><td>Tacos</td><td>Bring to boil</td></tr><tr><td>Tacos</td><td>Lower heat to simmer 5-10 minutes until desired consistency</td></tr><tr><td>Tacos</td><td>Put meat in tacos</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Use array() function to create a new array column by merging the data from multiple columns.\n\nfrom pyspark.sql.functions import array\n\ndisplay( data_df3.select(data_df3.title,array(data_df3.title,data_df3.col).alias(\"Dummy_array\")) )\n\n#display( data_df3.select(data_df3.title,array(data_df3.title,data_df3.col)[1].alias(\"Dummy_array\")) )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cb552d4-2b10-40da-8ba3-627c81c54499"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Tacos",["Tacos","Brown beef"]],["Tacos",["Tacos","Add taco seasoning and water, mix"]],["Tacos",["Tacos","Bring to boil"]],["Tacos",["Tacos","Lower heat to simmer 5-10 minutes until desired consistency"]],["Tacos",["Tacos","Put meat in tacos"]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"title","type":"\"string\"","metadata":"{}"},{"name":"Dummy_array","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>Dummy_array</th></tr></thead><tbody><tr><td>Tacos</td><td>List(Tacos, Brown beef)</td></tr><tr><td>Tacos</td><td>List(Tacos, Add taco seasoning and water, mix)</td></tr><tr><td>Tacos</td><td>List(Tacos, Bring to boil)</td></tr><tr><td>Tacos</td><td>List(Tacos, Lower heat to simmer 5-10 minutes until desired consistency)</td></tr><tr><td>Tacos</td><td>List(Tacos, Put meat in tacos)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n\ndata_schema = StructType(\n  [\n    StructField(\"Title\",StringType(),True),\n    StructField(\"Desc\",ArrayType(StringType()),True)\n  ]\n)\n\n\n\n# using StructType grammar we need a list as input data  thats why below conversion needed\nimport numpy as np\nx=(data_df2.collect()) # collect retrieves all elements in a DataFrame as an Array\ndata_df4 = spark.createDataFrame(data=x,schema=data_schema) # we use Array x as data input -- we cant use another dataframe as input\ndata_df4.printSchema()\n#display(data_df4)\n\nprint(x)\nprint(data_df4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7184aa79-bec9-4763-8274-ce27e59c77ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Title: string (nullable = true)\n |-- Desc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n[Row(title='Tacos', directions=['Brown beef', 'Add taco seasoning and water, mix', 'Bring to boil', 'Lower heat to simmer 5-10 minutes until desired consistency', 'Put meat in tacos'])]\nDataFrame[Title: string, Desc: array<string>]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Title: string (nullable = true)\n |-- Desc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n[Row(title='Tacos', directions=['Brown beef', 'Add taco seasoning and water, mix', 'Bring to boil', 'Lower heat to simmer 5-10 minutes until desired consistency', 'Put meat in tacos'])]\nDataFrame[Title: string, Desc: array<string>]\n"]}}],"execution_count":0},{"cell_type":"code","source":["\na1= data_df3.select(col(\"Title\")).toPandas()['Title'].tolist() # instead of using collect to convert , using toPandas and then using tolist to convert from dataframe to list\n\nprint(type(a1))\n\n# using collect func. bcz of retrieving all data , it can cause out of memory error in the case of big dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66154408-318c-46ca-965c-61cbf80a2fea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'list'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'list'>\n"]}}],"execution_count":0},{"cell_type":"code","source":["display(data_df3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6637793-1591-45e5-b54a-9051b563f047"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Tacos","Brown beef"],["Tacos","Add taco seasoning and water, mix"],["Tacos","Bring to boil"],["Tacos","Lower heat to simmer 5-10 minutes until desired consistency"],["Tacos","Put meat in tacos"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"title","type":"\"string\"","metadata":"{}"},{"name":"col","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>col</th></tr></thead><tbody><tr><td>Tacos</td><td>Brown beef</td></tr><tr><td>Tacos</td><td>Add taco seasoning and water, mix</td></tr><tr><td>Tacos</td><td>Bring to boil</td></tr><tr><td>Tacos</td><td>Lower heat to simmer 5-10 minutes until desired consistency</td></tr><tr><td>Tacos</td><td>Put meat in tacos</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n\ndata_schema = StructType(\n  [\n    StructField(\"Title\",StringType(),True),\n    StructField(\"Desc\",StringType(),True)\n  ]\n)\n\n\ny=data_df3.select(col(\"Title\"),col('col')).toPandas().values.tolist() # instead of collect we use toPandas and values to covert dataframe into a list\ndata_df4 = spStringTypeark.createDataFrame(data=y,schema=data_schema) # we use Array x as data input -- we cant use another dataframe as input\ndata_df4.printSchema()\n#display(data_df4)\n\nprint(x)\ndisplay(data_df4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bee3f1d-6c78-47e5-81fd-d13d4dc501f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3155039343297514>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_df3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Title\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'col'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# instead of collect we use toPandas and values to covert dataframe into a list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mdata_df4\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspStringTypeark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_schema\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# we use Array x as data input -- we cant use another dataframe as input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0mdata_df4\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;31m#display(data_df4)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'spStringTypeark' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'spStringTypeark' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3155039343297514>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_df3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Title\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'col'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# instead of collect we use toPandas and values to covert dataframe into a list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mdata_df4\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspStringTypeark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_schema\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# we use Array x as data input -- we cant use another dataframe as input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0mdata_df4\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;31m#display(data_df4)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'spStringTypeark' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["#using MapType  as input\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(),StringType()),True)\n])\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\ndf = spark.createDataFrame(data=dataDictionary, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\n\ndisplay( df.select(col('name') ,col('properties').eye.alias(\"eye\") , col('properties').hair.alias(\"hair\"))  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2f48ca8-e2f2-45d4-9b93-ca18e403d29d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#for loop implementation\nprint(type(df))\ndf2=df.select(col('name') ,col('properties').eye.alias(\"eye\") , col('properties').hair.alias(\"hair\"))\nprint(type(df2))\ndf3=df2.toPandas()\nprint(type(df3))\n\nfor index, row in df3.iterrows(): # itterrow only works with pandas.dataframe\n    print(\"\\n\")\n    print(index)\n    print(row['name'], row['hair'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96e1e7d0-6262-44b6-8b93-c1a00c88ea7e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["x=(df2.collect())  # collect function can work with <class 'pyspark.sql.dataframe.DataFrame'>\n\n#y=(df3.collect())  # if type is <class 'pandas.core.frame.DataFrame'> we cant use collect function\n\ndisplay(df2.sample(0.20) ) # to get 20% sample records"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e950453a-9774-4476-9607-b96cca799fd5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# working on Parquet File \n\n#Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem\n#While querying columnar storage, it skips the nonrelevant data very quickly, making faster query execution. \n#As a result aggregation queries consume less time compared to row-oriented databases.\n\n\n\ndata =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\ncolumns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\ndfp=spark.createDataFrame(data,columns)\n\n# write into parq. file\ndfp.write.mode('overwrite').parquet(\"/tmp/output/people.parquet\")\n\n# read from parq.\nparDF=spark.read.parquet(\"/tmp/output/people.parquet\")\n\n\ndisplay(parDF)\n\n#execute as sql\n\nparDF.createOrReplaceTempView(\"ParquetTable\")\nparkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n\n#CREATE TABLE USING PARQ. FILE\nspark.sql(\"CREATE or REPLACE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")\nspark.sql(\"SELECT * FROM PERSON\").show()\n\n\n#CREATE PARTITIONED PARQ. file\ndfp.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/tmp/output/people2.parquet\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05b11d81-9765-4a24-b640-36dc6f2e94b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#to_json() function is used to convert DataFrame columns MapType or Struct type to JSON string,,\n\n\n#display(df)\n\nprint( df.printSchema()  )\n\nfrom pyspark.sql.functions import to_json,col\n\ndf.withColumn(\"properties\",to_json(col(\"properties\"))).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95237919-b273-4239-94a2-f4a83a13476a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#overlay() Function\n#Replace column value with a string value from another column.\n\nfrom pyspark.sql.functions import overlay\ndf = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\ndf.select(overlay(\"col1\", \"col2\",0).alias(\"overlayed\")).show()\ndf.select(overlay(\"col1\", \"col2\",1).alias(\"overlayed\")).show()\ndf.select(overlay(\"col1\", \"col2\",6).alias(\"overlayed\")).show()\n\n\nx=df.select(overlay(\"col1\", \"col2\",6).alias(\"overlayed\")).toPandas().values.tolist() \n\nprint(type(x))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82c1de38-c5f7-4421-956d-25a95371da18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# read json example 2\nfrom pyspark.sql.types import *\n\n# Read multiline json file 1\ndata_df = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/tables/JSON/Ex1.json\")\ndisplay(data_df) \n\nprint(data_df.printSchema() )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e137e80-efee-4ed3-85ae-acd2ad18c210"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[[[["1001","Regular"],["1002","Chocolate"],["1003","Blueberry"],["1004","Devil's Food"]]],"0001","Cake",0.55,[["5001","None"],["5002","Glazed"],["5005","Sugar"],["5007","Powdered Sugar"],["5006","Chocolate with Sprinkles"],["5003","Chocolate"],["5004","Maple"]],"donut"],[[[["1001","Regular"]]],"0002","Raised",0.55,[["5001","None"],["5002","Glazed"],["5005","Sugar"],["5003","Chocolate"],["5004","Maple"]],"donut"],[[[["1001","Regular"],["1002","Chocolate"]]],"0003","Old Fashioned",0.55,[["5001","None"],["5002","Glazed"],["5003","Chocolate"],["5004","Maple"]],"donut"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"batters","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"batter\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"ppu","type":"\"double\"","metadata":"{}"},{"name":"topping","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>batters</th><th>id</th><th>name</th><th>ppu</th><th>topping</th><th>type</th></tr></thead><tbody><tr><td>List(List(List(1001, Regular), List(1002, Chocolate), List(1003, Blueberry), List(1004, Devil's Food)))</td><td>0001</td><td>Cake</td><td>0.55</td><td>List(List(5001, None), List(5002, Glazed), List(5005, Sugar), List(5007, Powdered Sugar), List(5006, Chocolate with Sprinkles), List(5003, Chocolate), List(5004, Maple))</td><td>donut</td></tr><tr><td>List(List(List(1001, Regular)))</td><td>0002</td><td>Raised</td><td>0.55</td><td>List(List(5001, None), List(5002, Glazed), List(5005, Sugar), List(5003, Chocolate), List(5004, Maple))</td><td>donut</td></tr><tr><td>List(List(List(1001, Regular), List(1002, Chocolate)))</td><td>0003</td><td>Old Fashioned</td><td>0.55</td><td>List(List(5001, None), List(5002, Glazed), List(5003, Chocolate), List(5004, Maple))</td><td>donut</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["#using MapType  as input\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType,ArrayType\nfrom pyspark.sql.functions import col\nschema = StructType([\n  StructField('id',StringType()),\n  StructField('batters', StructType([\n    \n        StructField(\n        'batter', ArrayType(\n            StructType([\n                StructField('id', StringType(), True),\n                StructField('type',StringType() , True) \n               \n                \n            ])\n        )\n    )\n    \n    \n  ])  )\n])\n\ndfx=data_df[[\"id\",\"batters\"]].toPandas().values.tolist()\n\n\ndf = spark.createDataFrame(data=dfx, schema = schema)\ndisplay(   df.select(col(\"id\"),col(\"batters\").batter.id,col(\"batters\").batter.type    )   )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3151d86b-a85f-4104-9f3b-df82ec40db92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0001",["1001","1002","1003","1004"],["Regular","Chocolate","Blueberry","Devil's Food"]],["0002",["1001"],["Regular"]],["0003",["1001","1002"],["Regular","Chocolate"]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"batters.batter.id","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"batters.batter.type","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>batters.batter.id</th><th>batters.batter.type</th></tr></thead><tbody><tr><td>0001</td><td>List(1001, 1002, 1003, 1004)</td><td>List(Regular, Chocolate, Blueberry, Devil's Food)</td></tr><tr><td>0002</td><td>List(1001)</td><td>List(Regular)</td></tr><tr><td>0003</td><td>List(1001, 1002)</td><td>List(Regular, Chocolate)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\ndf0=df.select(col(\"id\"),col(\"batters\").batter.id.alias(\"batter_id\"),col(\"batters\").batter.type.alias(\"batter_type\"))  \ndisplay(df0)\n#from pyspark.sql import functions as F\nfrom pyspark.sql.functions import arrays_zip, col, explode\n\n'''\nArray_zip() : zip() is a Array class method which Converts any arguments to arrays, then merges elements of self with corresponding elements from each argument.\n'''\n\ndf2 = df0.withColumn(\"new\", arrays_zip(\"batter_id\", \"batter_type\"))\\\n       .withColumn(\"new\", explode(\"new\"))\\\n       .select( col(\"id\"),col(\"new.batter_id\").alias(\"BATTER_ID\"), col(\"new.batter_type\").alias(\"BATTER_TYPE\"))\ndisplay(df2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34cb3717-6814-4d08-b66d-650915c8feaa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0001",["1001","1002","1003","1004"],["Regular","Chocolate","Blueberry","Devil's Food"]],["0002",["1001"],["Regular"]],["0003",["1001","1002"],["Regular","Chocolate"]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"batter_id","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"batter_type","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>batter_id</th><th>batter_type</th></tr></thead><tbody><tr><td>0001</td><td>List(1001, 1002, 1003, 1004)</td><td>List(Regular, Chocolate, Blueberry, Devil's Food)</td></tr><tr><td>0002</td><td>List(1001)</td><td>List(Regular)</td></tr><tr><td>0003</td><td>List(1001, 1002)</td><td>List(Regular, Chocolate)</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0001","1001","Regular"],["0001","1002","Chocolate"],["0001","1003","Blueberry"],["0001","1004","Devil's Food"],["0002","1001","Regular"],["0003","1001","Regular"],["0003","1002","Chocolate"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"BATTER_ID","type":"\"string\"","metadata":"{}"},{"name":"BATTER_TYPE","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>BATTER_ID</th><th>BATTER_TYPE</th></tr></thead><tbody><tr><td>0001</td><td>1001</td><td>Regular</td></tr><tr><td>0001</td><td>1002</td><td>Chocolate</td></tr><tr><td>0001</td><td>1003</td><td>Blueberry</td></tr><tr><td>0001</td><td>1004</td><td>Devil's Food</td></tr><tr><td>0002</td><td>1001</td><td>Regular</td></tr><tr><td>0003</td><td>1001</td><td>Regular</td></tr><tr><td>0003</td><td>1002</td><td>Chocolate</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(data_df.printSchema() )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5fd4012-aa8e-44bb-ad23-f011f3421760"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["#using MapType  as input\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType,ArrayType\nfrom pyspark.sql.functions import col\nschema = StructType([\n  StructField('id',StringType()),\n        StructField(\n        'topping', ArrayType(\n            StructType([\n                StructField('id', StringType(), True),\n                StructField('type',StringType() , True)      \n            ])\n        )\n    )\n  ] )\ndfy=data_df[[\"id\",\"topping\"]].toPandas().values.tolist()\n\ndf = spark.createDataFrame(data=dfy, schema = schema)\n\ndf1=  df.select(col(\"id\"),col(\"topping\").id.alias(\"TOP_ID\"),col(\"topping\").type.alias(\"TOP_TYPE\")    )   \n\ndisplay(df1)\n\ndf2 = df1.withColumn(\"new\", arrays_zip(\"TOP_ID\", \"TOP_TYPE\"))\\\n       .withColumn(\"new\", explode(\"new\"))\\\n       .select( col(\"id\"),col(\"new.TOP_ID\").alias(\"TOP_ID\"), col(\"new.TOP_TYPE\").alias(\"TOP_TYPE\"))\ndisplay(df2)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57077f9d-1283-4452-bf33-74556cc13226"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0001",["5001","5002","5005","5007","5006","5003","5004"],["None","Glazed","Sugar","Powdered Sugar","Chocolate with Sprinkles","Chocolate","Maple"]],["0002",["5001","5002","5005","5003","5004"],["None","Glazed","Sugar","Chocolate","Maple"]],["0003",["5001","5002","5003","5004"],["None","Glazed","Chocolate","Maple"]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"TOP_ID","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"TOP_TYPE","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>TOP_ID</th><th>TOP_TYPE</th></tr></thead><tbody><tr><td>0001</td><td>List(5001, 5002, 5005, 5007, 5006, 5003, 5004)</td><td>List(None, Glazed, Sugar, Powdered Sugar, Chocolate with Sprinkles, Chocolate, Maple)</td></tr><tr><td>0002</td><td>List(5001, 5002, 5005, 5003, 5004)</td><td>List(None, Glazed, Sugar, Chocolate, Maple)</td></tr><tr><td>0003</td><td>List(5001, 5002, 5003, 5004)</td><td>List(None, Glazed, Chocolate, Maple)</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0001","5001","None"],["0001","5002","Glazed"],["0001","5005","Sugar"],["0001","5007","Powdered Sugar"],["0001","5006","Chocolate with Sprinkles"],["0001","5003","Chocolate"],["0001","5004","Maple"],["0002","5001","None"],["0002","5002","Glazed"],["0002","5005","Sugar"],["0002","5003","Chocolate"],["0002","5004","Maple"],["0003","5001","None"],["0003","5002","Glazed"],["0003","5003","Chocolate"],["0003","5004","Maple"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"TOP_ID","type":"\"string\"","metadata":"{}"},{"name":"TOP_TYPE","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>TOP_ID</th><th>TOP_TYPE</th></tr></thead><tbody><tr><td>0001</td><td>5001</td><td>None</td></tr><tr><td>0001</td><td>5002</td><td>Glazed</td></tr><tr><td>0001</td><td>5005</td><td>Sugar</td></tr><tr><td>0001</td><td>5007</td><td>Powdered Sugar</td></tr><tr><td>0001</td><td>5006</td><td>Chocolate with Sprinkles</td></tr><tr><td>0001</td><td>5003</td><td>Chocolate</td></tr><tr><td>0001</td><td>5004</td><td>Maple</td></tr><tr><td>0002</td><td>5001</td><td>None</td></tr><tr><td>0002</td><td>5002</td><td>Glazed</td></tr><tr><td>0002</td><td>5005</td><td>Sugar</td></tr><tr><td>0002</td><td>5003</td><td>Chocolate</td></tr><tr><td>0002</td><td>5004</td><td>Maple</td></tr><tr><td>0003</td><td>5001</td><td>None</td></tr><tr><td>0003</td><td>5002</td><td>Glazed</td></tr><tr><td>0003</td><td>5003</td><td>Chocolate</td></tr><tr><td>0003</td><td>5004</td><td>Maple</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Data Op","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2033568028168007}},"nbformat":4,"nbformat_minor":0}
